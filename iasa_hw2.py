{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":471627,"sourceType":"datasetVersion","datasetId":212391}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:13:33.836375Z\",\"iopub.execute_input\":\"2025-04-06T12:13:33.836607Z\",\"iopub.status.idle\":\"2025-04-06T12:13:40.440456Z\",\"shell.execute_reply.started\":\"2025-04-06T12:13:33.836586Z\",\"shell.execute_reply\":\"2025-04-06T12:13:40.439122Z\"}}\n!pip install jiwer\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:13:40.441808Z\",\"iopub.execute_input\":\"2025-04-06T12:13:40.442213Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.477903Z\",\"shell.execute_reply.started\":\"2025-04-06T12:13:40.442174Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.477230Z\"}}\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import Wav2Vec2Model, HubertModel, Wav2Vec2Processor, AutoModel, AutoProcessor\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom jiwer import wer\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport torchaudio\nimport pytorch_lightning as pl\nimport wandb\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\nfrom kaggle_secrets import UserSecretsClient\n\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.478607Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.478835Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.506575Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.478815Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.505661Z\"}}\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.508504Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.508720Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.670637Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.508703Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.669969Z\"}}\n# Constants\nTIMIT_ROOT = \"/kaggle/input/darpa-timit-acousticphonetic-continuous-speech/data\"  # Update this with your TIMIT dataset path\nBATCH_SIZE = 32        # Reduced batch size to avoid memory issues\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 5        # Reduced for faster testing, increase for better results\nMAX_LENGTH = 160000   # Max audio length in samples (~10 seconds at 16kHz)\n\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb_key\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.671752Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.672043Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.679604Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.672014Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.678535Z\"}}\n# Define phoneme mapping (61 to 39 phonemes mapping commonly used for TIMIT)\nPHONEME_MAP = {\n    'aa': 'aa', 'ae': 'ae', 'ah': 'ah', 'ao': 'aa', 'aw': 'aw', 'ax': 'ah', 'ax-h': 'ah',\n    'axr': 'er', 'ay': 'ay', 'b': 'b', 'bcl': 'sil', 'ch': 'ch', 'd': 'd', 'dcl': 'sil',\n    'dh': 'dh', 'dx': 'd', 'eh': 'eh', 'el': 'l', 'em': 'm', 'en': 'n', 'eng': 'ng',\n    'epi': 'sil', 'er': 'er', 'ey': 'ey', 'f': 'f', 'g': 'g', 'gcl': 'sil', 'h#': 'sil',\n    'hh': 'hh', 'hv': 'hh', 'ih': 'ih', 'ix': 'ih', 'iy': 'iy', 'jh': 'jh', 'k': 'k',\n    'kcl': 'sil', 'l': 'l', 'm': 'm', 'n': 'n', 'ng': 'ng', 'nx': 'n', 'ow': 'ow',\n    'oy': 'oy', 'p': 'p', 'pau': 'sil', 'pcl': 'sil', 'q': 'sil', 'r': 'r', 's': 's',\n    'sh': 'sh', 't': 't', 'tcl': 'sil', 'th': 'th', 'uh': 'uh', 'uw': 'uw', 'ux': 'uw',\n    'v': 'v', 'w': 'w', 'y': 'y', 'z': 'z', 'zh': 'sh'\n}\n\n# Create a mapping from phoneme to index\nPHONEME_TO_IDX = {phoneme: idx for idx, phoneme in enumerate(sorted(set(PHONEME_MAP.values())))}\nIDX_TO_PHONEME = {idx: phoneme for phoneme, idx in PHONEME_TO_IDX.items()}\nNUM_PHONEMES = len(PHONEME_TO_IDX)\n\nprint(f\"Number of phonemes: {NUM_PHONEMES}\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.680620Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.680855Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.698886Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.680835Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.698071Z\"}}\nclass TimitDataset(Dataset):\n    def __init__(self, root_dir, split='train', ssl_model_name=\"facebook/hubert-large-ls960-ft\", layer_idx=-1):\n        \"\"\"\n        Args:\n            root_dir: Path to TIMIT dataset\n            split: 'train' or 'test'\n            ssl_model_name: Name of the SSL model to use\n            layer_idx: Index of the layer to use from SSL model (-1 for last_hidden_state)\n        \"\"\"\n        self.root_dir = root_dir\n        self.split = split\n        self.layer_idx = layer_idx\n        \n        # Load SSL model and processor based on model name\n        if \"hubert\" in ssl_model_name:\n            self.ssl_model = HubertModel.from_pretrained(ssl_model_name).to(device)\n            self.processor = AutoProcessor.from_pretrained(ssl_model_name)\n        elif \"wav2vec2\" in ssl_model_name:\n            self.ssl_model = Wav2Vec2Model.from_pretrained(ssl_model_name).to(device)\n            self.processor = Wav2Vec2Processor.from_pretrained(ssl_model_name)\n        else:\n            self.ssl_model = AutoModel.from_pretrained(ssl_model_name).to(device)\n            self.processor = AutoProcessor.from_pretrained(ssl_model_name)\n        \n        # Freeze SSL model parameters\n        for param in self.ssl_model.parameters():\n            param.requires_grad = False\n            \n        # Get file paths\n        split_dir = os.path.join(root_dir, split.upper())\n        self.audio_files = []\n        self.phoneme_files = []\n        \n        for dialect in os.listdir(split_dir):\n            dialect_dir = os.path.join(split_dir, dialect)\n            if os.path.isdir(dialect_dir):\n                for speaker in os.listdir(dialect_dir):\n                    speaker_dir = os.path.join(dialect_dir, speaker)\n                    if os.path.isdir(speaker_dir):\n                        for file in os.listdir(speaker_dir):\n                            if file.endswith('.WAV'):\n                                audio_path = os.path.join(speaker_dir, file)\n                                phoneme_path = os.path.join(speaker_dir, file.replace('.WAV', '.PHN'))\n                                if os.path.exists(phoneme_path):\n                                    self.audio_files.append(audio_path)\n                                    self.phoneme_files.append(phoneme_path)\n        \n        print(f\"Found {len(self.audio_files)} audio files in {split} set\")\n        print(f\"Found {len(self.phoneme_files)} audio files in {split} set\")\n    \n    def __len__(self):\n        return len(self.audio_files)\n    \n    def __getitem__(self, idx):\n        audio_path = self.audio_files[idx]\n        phoneme_path = self.phoneme_files[idx]\n        \n        # Load audio and preprocess\n        waveform, sample_rate = torchaudio.load(audio_path)\n        waveform = waveform[0]  # Take the first channel\n        \n        # Pad or truncate\n        if waveform.shape[0] < MAX_LENGTH:\n            padded = torch.zeros(MAX_LENGTH)\n            padded[:waveform.shape[0]] = waveform\n            waveform = padded\n        else:\n            waveform = waveform[:MAX_LENGTH]\n        \n        # Extract SSL features\n        inputs = self.processor(waveform, sampling_rate=sample_rate, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = self.ssl_model(**inputs, output_hidden_states=True)\n        \n        # Get features from the specified layer\n        if self.layer_idx == -1:\n            features = outputs.last_hidden_state.squeeze(0)\n        else:\n            features = outputs.hidden_states[self.layer_idx].squeeze(0)\n        \n        # Load phoneme labels\n        phonemes = []\n        with open(phoneme_path, 'r') as f:\n            for line in f:\n                start, end, phoneme = line.strip().split()\n                start, end = int(start), int(end)\n                # Map to reduced phoneme set\n                mapped_phoneme = PHONEME_MAP.get(phoneme.lower(), 'sil')\n                phonemes.append((start, end, mapped_phoneme))\n        \n        # Convert to frame-level phoneme labels\n        # Calculate frames per sample\n        frames_per_sample = features.shape[0]\n        samples_per_frame = waveform.shape[0] / frames_per_sample\n        \n        frame_labels = torch.zeros(frames_per_sample, dtype=torch.long)\n        for start, end, phoneme in phonemes:\n            start_frame = min(int(start / samples_per_frame), frames_per_sample - 1)\n            end_frame = min(int(end / samples_per_frame), frames_per_sample - 1)\n            if start_frame <= end_frame:\n                frame_labels[start_frame:end_frame+1] = PHONEME_TO_IDX[phoneme]\n        \n        return features.cpu(), frame_labels\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.699701Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.699947Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.716467Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.699928Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.715611Z\"}}\n# Define model heads as before\nclass LinearHead(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LinearHead, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        return self.fc(x)\n\nclass MLPHead(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.3):\n        super(MLPHead, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\nclass LSTMHead(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, dropout=0.3):\n        super(LSTMHead, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n                           batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x, _ = self.lstm(x)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\nclass ConvHead(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=3, dropout=0.3):\n        super(ConvHead, self).__init__()\n        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # Change from [batch, seq_len, features] to [batch, features, seq_len] for conv1d\n        x = x.transpose(1, 2)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        # Change back to [batch, seq_len, features] for linear layer\n        x = x.transpose(1, 2)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\nclass AttentionHead(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=4, dropout=0.3):\n        super(AttentionHead, self).__init__()\n        self.norm1 = nn.LayerNorm(input_dim)\n        self.attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout)\n        self.norm2 = nn.LayerNorm(input_dim)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, input_dim)\n        self.final_fc = nn.Linear(input_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        # Self-attention block\n        res = x\n        x = self.norm1(x)\n        # Convert to [seq_len, batch, hidden_dim] for attention\n        x_t = x.transpose(0, 1)\n        x_t, _ = self.attention(x_t, x_t, x_t)\n        # Back to [batch, seq_len, hidden_dim]\n        x = x_t.transpose(0, 1)\n        x = res + self.dropout(x)\n        \n        # Feed-forward block\n        res = x\n        x = self.norm2(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = res + self.dropout(x)\n        \n        # Final projection\n        x = self.final_fc(x)\n        return x\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.717105Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.717369Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.727738Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.717350Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.726873Z\"}}\ndef collate_fn(batch):\n    features, labels = zip(*batch)\n    \n    # Find the minimum sequence length in this batch\n    min_len = min(f.shape[0] for f in features)\n    \n    # Truncate sequences to min_len\n    features = [f[:min_len] for f in features]\n    labels = [l[:min_len] for l in labels]\n    \n    # Stack along batch dimension\n    features = torch.stack(features)\n    labels = torch.stack(labels)\n    \n    return features, labels\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:14:04.728440Z\",\"iopub.execute_input\":\"2025-04-06T12:14:04.728701Z\",\"iopub.status.idle\":\"2025-04-06T12:14:04.743687Z\",\"shell.execute_reply.started\":\"2025-04-06T12:14:04.728672Z\",\"shell.execute_reply\":\"2025-04-06T12:14:04.742861Z\"}}\n# PyTorch Lightning implementation\nclass PhonemeRecognitionModel(pl.LightningModule):\n    def __init__(self, model, lr=LEARNING_RATE, weight_decay=1e-5):\n        super().__init__()\n        self.model = model\n        self.lr = lr\n        self.weight_decay = weight_decay\n        \n        # Define loss function - CTC loss could be a more appropriate choice than CrossEntropyLoss\n        # for sequence-to-sequence tasks, but keeping CrossEntropyLoss for now since \n        # we're working with frame-level classification\n        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)  # Ignore padding\n        \n        # Track metrics\n        self.train_losses = []\n        self.val_losses = []\n        self.val_pers = []\n\n        # Storage for validation outputs\n        self._val_step_outputs = []\n        \n        # Storage for test outputs\n        self._test_step_outputs = []\n\n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(\n            self.parameters(), \n            lr=self.lr, \n            weight_decay=self.weight_decay\n        )\n        scheduler = {\n            'scheduler': optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer, mode='min', factor=0.5, patience=2, verbose=True\n            ),\n            'monitor': 'val_loss',\n            'interval': 'epoch',\n            'frequency': 1\n        }\n        return [optimizer], [scheduler]\n    \n    def training_step(self, batch, batch_idx):\n        features, labels = batch\n        outputs = self(features)\n        \n        # Reshape for loss calculation [batch*seq_len, num_classes]\n        outputs_flat = outputs.view(-1, NUM_PHONEMES)\n        labels_flat = labels.view(-1)\n        \n        loss = self.criterion(outputs_flat, labels_flat)\n        \n        # Log metrics\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        features, labels = batch\n        outputs = self(features)\n        \n        # Calculate loss\n        outputs_flat = outputs.view(-1, NUM_PHONEMES)\n        labels_flat = labels.view(-1)\n        loss = self.criterion(outputs_flat, labels_flat)\n        \n        # Get predictions\n        preds = torch.argmax(outputs, dim=2)\n        \n        # Store for epoch-end processing\n        self._val_step_outputs.append({\n            'preds': preds.detach().cpu(),\n            'labels': labels.detach().cpu()\n        })\n        \n        # Log validation loss\n        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    def on_validation_epoch_end(self):\n        # Extract all predictions and labels\n        all_preds = [item['preds'] for item in self._val_step_outputs]\n        all_labels = [item['labels'] for item in self._val_step_outputs]\n        \n        # Convert lists of tensors to numpy arrays\n        all_preds_np = [p.numpy() for p in all_preds]\n        all_labels_np = [l.numpy() for l in all_labels]\n        \n        # Calculate PER over the whole validation set\n        per = self.calculate_per(all_preds_np, all_labels_np)\n        \n        # Log the validation PER\n        self.log('val_per', per, prog_bar=True)\n        \n        # Clear the outputs list to free memory\n        self._val_step_outputs.clear()\n    \n    def test_step(self, batch, batch_idx):\n        features, labels = batch\n        outputs = self(features)\n        \n        # Calculate loss\n        outputs_flat = outputs.view(-1, NUM_PHONEMES)\n        labels_flat = labels.view(-1)\n        loss = self.criterion(outputs_flat, labels_flat)\n        \n        # Get predictions\n        preds = torch.argmax(outputs, dim=2)\n        \n        # Store for epoch-end processing\n        self._test_step_outputs.append({\n            'test_loss': loss.detach().cpu(),\n            'preds': preds.detach().cpu(),\n            'labels': labels.detach().cpu()\n        })\n        \n        # Log test loss\n        self.log('test_loss', loss, on_epoch=True)\n        \n        return loss\n    \n    def on_test_epoch_end(self):\n        # Calculate average test loss\n        avg_loss = torch.stack([x['test_loss'] for x in self._test_step_outputs]).mean()\n        \n        # Extract all predictions and labels\n        all_preds = [item['preds'] for item in self._test_step_outputs]\n        all_labels = [item['labels'] for item in self._test_step_outputs]\n        \n        # Convert lists of tensors to numpy arrays\n        all_preds_np = [p.numpy() for p in all_preds]\n        all_labels_np = [l.numpy() for l in all_labels]\n        \n        # Calculate PER over the whole test set\n        per = self.calculate_per(all_preds_np, all_labels_np)\n        \n        # Log the test PER\n        self.log('test_per', per)\n        print(f\"Test PER: {per:.4f}\")\n        \n        # Clear the outputs list to free memory\n        self._test_step_outputs.clear()\n    \n    @staticmethod\n    def calculate_per(predictions, labels):\n        \"\"\"Calculate Phoneme Error Rate - improved implementation using edit distance\"\"\"\n        # Convert to strings of phonemes for WER calculation\n        pred_strings = []\n        label_strings = []\n        \n        for pred_batch, label_batch in zip(predictions, labels):\n            for pred_seq, label_seq in zip(pred_batch, label_batch):\n                # Convert indices to phoneme names\n                pred_phonemes = [IDX_TO_PHONEME[p] for p in pred_seq]\n                label_phonemes = [IDX_TO_PHONEME[l] for l in label_seq]\n                \n                # Join into space-separated strings\n                pred_strings.append(' '.join(pred_phonemes))\n                label_strings.append(' '.join(label_phonemes))\n        \n        # Calculate WER (which is equivalent to PER for phoneme sequences)\n        error_rate = wer(label_strings, pred_strings)\n        return error_rate\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2025-04-06T12:18:08.280654Z\",\"iopub.execute_input\":\"2025-04-06T12:18:08.281016Z\",\"execution_failed\":\"2025-04-06T12:30:10.695Z\"}}\ndef main():\n    # Initialize wandb\n    wandb.login(key=wandb_key)\n    \n    # SSL model configurations to try\n    ssl_configs = [\n        {\"name\": \"facebook/hubert-large-ls960-ft\", \"layer_idx\": -1},\n        {\"name\": \"facebook/hubert-large-ls960-ft\", \"layer_idx\": 4},\n        {\"name\": \"facebook/hubert-large-ls960-ft\", \"layer_idx\": 6},   # Middle layer        \n        {\"name\": \"facebook/wav2vec2-base\", \"layer_idx\": -1},      # Last hidden state\n        {\"name\": \"facebook/wav2vec2-base\", \"layer_idx\": 4},        # Middle layer\n        {\"name\": \"facebook/wav2vec2-base\", \"layer_idx\": 6},        # Middle layer\n    ]\n    \n    # Head configurations to try\n    hidden_dim = 256  # Size of intermediate layers - Balance between capacity and efficiency\n    \n    # Results table\n    results = []\n    \n    for ssl_config in ssl_configs:\n        ssl_name = ssl_config[\"name\"].split('/')[-1]\n        layer_name = f\"layer_{ssl_config['layer_idx']}\" if ssl_config['layer_idx'] != -1 else \"last_layer\"\n        \n        print(f\"\\n===== Testing {ssl_name} with {layer_name} =====\")\n        \n        # Create datasets\n        train_dataset = TimitDataset(TIMIT_ROOT, 'train', ssl_config[\"name\"], ssl_config[\"layer_idx\"])\n        test_dataset = TimitDataset(TIMIT_ROOT, 'test', ssl_config[\"name\"], ssl_config[\"layer_idx\"])\n        \n        # Split train into train and validation\n        train_size = int(0.9 * len(train_dataset))\n        val_size = len(train_dataset) - train_size\n        train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n        \n        # Create data loaders\n        train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n        \n        # Get feature dimension from a sample\n        sample_features, _ = train_dataset[0]\n        feature_dim = sample_features.shape[1]\n        print(f\"Feature dimension: {feature_dim}\")\n        \n        # Try different heads\n        heads = {\n            \"Linear\": LinearHead(feature_dim, NUM_PHONEMES),\n            \"MLP\": MLPHead(feature_dim, hidden_dim, NUM_PHONEMES),\n            \"LSTM\": LSTMHead(feature_dim, hidden_dim, NUM_PHONEMES),\n            \"Conv\": ConvHead(feature_dim, hidden_dim, NUM_PHONEMES),\n            \"Attention\": AttentionHead(feature_dim, hidden_dim, NUM_PHONEMES)\n        }\n        \n        for head_name, head_model in heads.items():\n            print(f\"\\n--- Training with {head_name} head ---\")\n            \n            # Initialize wandb run\n            run_name = f\"{ssl_name}_{layer_name}_{head_name}\"\n            wandb_logger = WandbLogger(project=\"timit_phoneme_recognition\", name=run_name)\n            \n            # Configure model with PyTorch Lightning\n            model = PhonemeRecognitionModel(head_model, lr=LEARNING_RATE)\n            \n            # Setup callbacks\n            checkpoint_callback = ModelCheckpoint(\n                monitor='val_per',\n                dirpath=f'checkpoints/{run_name}',\n                filename='{epoch:02d}-{val_per:.4f}',\n                save_top_k=1,\n                mode='min'\n            )\n            \n            early_stop_callback = EarlyStopping(\n                monitor='val_per',\n                patience=5,\n                mode='min'\n            )\n            \n            lr_monitor = LearningRateMonitor(logging_interval='epoch')\n            \n            # Setup trainer\n            trainer = pl.Trainer(\n                max_epochs=NUM_EPOCHS,\n                accelerator='auto',\n                devices=1 if torch.cuda.is_available() else None,\n                logger=wandb_logger,\n                callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n                log_every_n_steps=10\n            )\n            \n            # Train model\n            trainer.fit(model, train_loader, val_loader)\n            \n            # Test model using best checkpoint\n            trainer.test(ckpt_path=checkpoint_callback.best_model_path, dataloaders=test_loader)\n            \n            # Get test PER\n            test_results = trainer.test(model, dataloaders=test_loader)\n            test_per = test_results[0]['test_per']\n            \n            # Record results\n            results.append({\n                \"SSL Model\": ssl_name,\n                \"Layer\": layer_name,\n                \"Head\": head_name,\n                \"Val PER\": model.val_pers[-1] if model.val_pers else float('nan'),\n                \"Test PER\": test_per\n            })\n            \n            # Finish wandb run\n            wandb.finish()\n    \n    # Create results table\n    results_df = pd.DataFrame(results)\n    print(\"\\nResults Table:\")\n    print(results_df)\n    results_df.to_csv(\"phoneme_recognition_results.csv\", index=False)\n    \n    # Plot results\n    plt.figure(figsize=(12, 8))\n    for ssl_name in results_df[\"SSL Model\"].unique():\n        for layer_name in results_df[\"Layer\"].unique():\n            data = results_df[(results_df[\"SSL Model\"] == ssl_name) & (results_df[\"Layer\"] == layer_name)]\n            if not data.empty:\n                plt.plot(data[\"Head\"], data[\"Test PER\"], marker='o', label=f\"{ssl_name}_{layer_name}\")\n    \n    plt.xlabel(\"Head Type\")\n    plt.ylabel(\"Test PER (lower is better)\")\n    plt.title(\"Phoneme Error Rate by Model Configuration\")\n    plt.legend()\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(\"phoneme_recognition_results.png\")\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"c0a8a9b0-483b-4d2d-898d-7af7571d504e","_cell_guid":"d30b2f7b-6a49-46fa-9916-449b087a623d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}